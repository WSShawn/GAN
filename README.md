# Generative Adversarial Networks

## Introduction and concept

Generative Adversarial Networks (GANs) represent a class of Deep Learning Frameworks which have firstly been introduced by Ian Goodfellow in 2014. While their spectrum of application is becoming larger with research, the main focus of GANs has been image processing.

The main idea behind GANs consists in the competition between 2 neural networks in a zero-sum game, the scope of each network being to improve its prediction accuracy. One network is named the "Generator" and the other the "Discriminator". The Generator's goal is to create fake data, with the ultimate goal of its output to be perceived as real. The goal of the Discrimnator is to look at the data generated by the Generator and label the data as "real" or "fake". As the data that the application of Generative Adversarial Networks is mainly focused on, as well as the data in our study is represented by images, we will focus on implementing Deep Convolutional Generative Adversarial Networks (DCGANs).

## Data 

The dataset used in our study is the Celeba Dataset. It represents a collection of more than 200K photos of celebrities, which come in .jpg format. We can plot some of the images in the training set :
![image](https://user-images.githubusercontent.com/114659655/208944403-5f22e5f0-5290-4fa2-80c8-6811f8318fec.png)




## Deep Convolutional Generative Adversarial networks 

From a general point of view, Convolutional Neural Networks represent a particular type of neural networks that posess an ability to detect patterns in the data (for example, shapes, corners or edges in the case of images). Their hidden layers, called "Convolutional Layers", receive input, transform it and then pass it to the following layers. The operation that corresponds to the transformation is called a "Convolution".

### Convolution

The object at the basis of the convolution process is the kernel. Given the 3-dimensional characterisation of our data (height, width and values of the RGB channel), the kernel can be initialized as 3-dimensional, with the 3rd dimension representing the number of filters applied, or 2-dimensional, meaning that the same filter will be applied to each of the 3 color channels. The values that constitute the kernel represent the weights of the network, they are randomly initialized and will be further optimized according to the accuracy and loss of the convolution process.

The kernel is then passed along the image, row by row and column by column, until it has slid over all the pixels of the image. A dot product is applied between the kernel and the surface on the image on which it slides. The resulting image is the output matrix. In the case of a Convolutional Neural Network, the output obtained by the convolution inside a hidden layer is passed as input to the next hidden layer.

![convolution1](https://user-images.githubusercontent.com/114659655/200131876-4e9d595c-9ff9-4f57-9c58-b13382168a12.png)

### Convolutional Layers

The neural networks we are going to build are going to be formed of Convolutional Layers. Convolutional Layers are meant to process data that is correlated in space. The output of each convolutional layer is a feature map, which corresponds to a spatial projection where certain features are exposed, which have previously been found by the convolutional layer.

The layers are interconnected : the second layer takes as input the output from the first layer and so on. This can be seen in the values of the arguments that are passed onto each layer. For example, the number of output channels from the first layer will be the number of input channels of the second layer. We implement the layers using they torch.nn module.


## Generator and Discriminator

Throughout the implementation of our neural networks, we wil use the Pytorch framework. Both the Generator and the Discriminator are Neural Network Object Types. They are defined as classes who inherit from the nn.Module in Pytorch. The structure of both networks will be defined using the __init__() method and applied to computation using the forward() method.


### Generator

The Generator Neural Network's initial input is represented by a latent 1-dimensional noise vector. The length of the latent vector is fixed and can be chosen arbitrarily, but we will choose 100 for our example. We will draw the latent vector from the Gaussian distribution. The dimension of the latent vector will represent the number of channels of the input of the first layer of the network. 

![1_ULAGAYoGGr5eEB2377ArYA](https://user-images.githubusercontent.com/114659655/201658929-c53960f3-1d5d-4e33-b6c6-f6f88d484100.png)


Transposed convolution is then applied by sliding the kernel along the noise vector. We need to pass from a 1x100 dimensional vector to the size of the images in the dataset (3x64x64), the convolution applied is transposed in order to upsample the data. Each transposed convolution will produce a feature map inside the generator.

Batch Normalization : After the Convolutional Layer, a batch normalization layer is implemented. It normalizes data at batch level so that it can be passed through the activation function afterwards. We are applying Batch Normalization to 2d images using BatchNorm2d().

### Discriminator

The input of the discriminator is an image in its intial dimensions. The image is processed through Conv2d layers, as the data has to be downsampled. As mentioned before, the filter (Kernel) passes through the set of pixels of the image. Values of the corresponding pixels are multiplied together then summed up to result in the convolved feature. 

The last convolution layer of the discriminator is flattened and passed through a sigmoid function. The Discriminator will therefore output the label of the image, 0 corresponding for 'fake' and 1 corresponding for 'real. The following image illustrates the architecture of the Discriminator Network.

![image](https://user-images.githubusercontent.com/114659655/208300570-d18620da-22cf-4d00-a430-b6293ade3069.png)




## Weights initializiation

Model weights will be initialized according to the paper by Radford, Metz and Chintala (2015). They are normally distributed with mean 0 and standard deviation 0.02. The function weights_initialization takes as input one of the 2 models and transforms input data within the network's hidden layers, respectively the Convolutional, Transposed Convolutional and Batch Normalization Layer.

For a Convolutional Layer, weights represent each matrix element in the Kernel, that will be trained.

```
def weights_initialization(model):
  for module in model.modules():
    if module == nn.Conv2d or module == nn.ConvTranspose2d or module == nn.BatchNorm2d :
      nn.init.normal_(model.weight.data, 0.0, 0.02)
```


# Training setup 

After the Discriminator and the Generator have been initialized given the hyperparameters defined at the beginning of the code (number of channels and number of features for the discriminator and size of the latent vector, the number of channels and the number of features for the generator), weights are initialized. We use the Binary Cross Entropy loss function and Adam Optimizer for both Networks. In order to understand the choice for these characteristics, we have to look at the principle of interaction between the networks :

![image](https://user-images.githubusercontent.com/114659655/208265282-205b6375-2370-408f-998e-27ed7c864ba5.png)

Here D(G(z)) is the probability that the output of the Generator G is classified as a real image. Let us look now at the Binary Cross Entropy Loss : 

![image](https://user-images.githubusercontent.com/114659655/208265308-6345ba98-1718-48be-9d65-2294af4cc528.png)

It can be noticed that the 2 functions look alike. Indeed, the value of y can be dictated so as to correspond to the objective function of each of the 2 networks. For the Generator, we are looking for min log(1 - D(G(z))) which is equivalent to max log(D(G(z)), whereas for the Discriminator we are looking for max log(D(x)) + log(1-D(G(z)). 

It is worth reminding that the function BCELoss measures the Binary Cross Entropy between an input tensor and a target tensor. In our model, labelling is done with the value 1 for images classified as real and 0 for images classified as fake by the discriminator, through the functions $$zeros_like$$ and $$ones_like$$. The values are put in a tensor, which is compared to a tensor of the same size filled with 1's through the BCE Loss criterion. The difference between the 2 network consists in the tensor used as input for the BCELoss function.

The Generator is trained to minimize the loss with respect to the fake images it generates, while the Discriminator's loss has 2 components as we have seen in its formula. We are maximizing the probability that it labels real images as real and fake images as fake. Through each batch the following operations are applied :


```
#Uploading real image to gpu
    real_image = real_image.to(device)

    #Generating noise vectors
    noise = torch.randn((batch_size, size_latent, 1, 1)).to(device)
    
    #Generating fake image from noise
    fake = gen(noise)

    #Train discriminator

    #Discriminator on real image
    disc_real = disc(real). reshape(-1)
    loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))

    #Discriminator on fake image
    disc_fake = disc(fake).reshape(-1)
    loss_disc_fake = criterion(fake, torch.zeros_like(disc_fake))

    total_loss = loss_disc_real + loss_disc_fake
    print("Discriminator Loss : " + str(total_loss))
    D_losses.append(total_loss)

    #Discriminator Adam optimization
    disc.zero_grad()

    #Retaining the graph as fake discriminated fake will be reutilized
    total_loss.backward(retain_graph = True) 

    disc_optimizer.step()


    #Train generator

    #Discriminate fake and get loss
    output = disc(fake).reshape(-1)
    generator_loss = criterion(output, torch.ones_like(output))
    print("Generator Loss :" + str(generator_loss))
    G_losses.append(generator_loss)

    #Generator Adam optimization
    gen.zero_grad()
    generator_loss.backward()
    gen_optimizer.step()

```



#Extension : WGAN- Wasserstein Generative Adversarial Networks

We have seen the principles of GANs and the functioning of DCGANS. We are now focusing on Wasserstein Generative Adversarial Networks


## Introduction and concept

Firstly let us understand why such variations of the traditional GANs have been created. While a very performant tool, GANs can be subject to convergence failure (failure to produce optimal results) and mode collapse (model failing to produce unique results and repeating a similar pattern, quality or classes). The advantage of WGANs is that they solve this issue and they offer higher stability for the training in comparison to traditional GANs. Another advantage is that the value of the global loss is meaningful, in the sense that it gives us a termination cirterion.

We have seen that the main idea in GAN implementation is that we have 2 probability distributions, one for the Generator Pg and one for the Discriminator Pd. The goal is to have similar probability distributions so that generated images look realistic. The issue here is therefore how we define this similarity, or the "distance" between the 2 distributions. In the case of WGAN's, we are focusing on Wasserstein distance measure.



