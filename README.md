# Generative Adversarial Networks

## Introduction and concept

Generative Adversarial Networks (GANs) represent a class of Deep Learning Frameworks which have firstly been introduced by Ian Goodfellow in 2014. While their spectrum of application is becoming larger with research, the main focus of GANs has been image processing.

The main idea behind GANs consists in the competition between 2 neural networks in a zero-sum game, the scope of each network being to improve its prediction accuracy. One network is named the "Generator" and the other the "Discriminator". The Generator's goal is to create fake data, with the ultimate goal of its output to be perceived as real. The goal of the Discrimnator is to look at the data generated by the Generator and label the data as "real" or "fake". As the data that the application of Generative Adversarial Networks is mainly focused on, as well as the data in our study, is represented by images, we will focus on implementing Deep Convolutional Generative Adversarial Networks (DCGANs), a subset of Convolutional Generative Adversarial Networks presenting a set of architecture constraints firstly introduced by Radford A., Metz  and L. Chintala S. in 2016.


## General Functioning of a Neural Network

Neural Networks represent a data processing algorithm inspired by the functioning of a human brain. They are generally composed of 3 groups of layers :

- Input layers : represents the information that is fed to the network in its raw state. This information can be numeric or images
- Hidden layers : are destined for the data processing. The hidden layers do the computations and extract information for the data, which can come in the form of features in the case of images (shapes, edges). The further the hidden layer, the more complex its identification task is. For example, first hidden layers can recognize shapes in an image, while further hidden layers can recognize entire objects
- Output layers : output the final predictions based on the previous layers' processing

Each layer of the mentioned layers is formed of neurons. This structure has been designed to mimick behavior of a human neuron. The neurons receive input, either from the previous layer or from the initial dataset. Afterwards, the input is processed through the multiplication with weights (process which can be associated with the signals produced in the dendrites of the neuron) and passed through an activation function, which will provide an output. 

The activation functions allow communication between the neurons through the synapses. The activation functions can be of different types, with respect to the desired output. Some examples include the Sigmoid function, Hyperbolic Tangent or Rectified Linear Unit (ReLU) functions. Each type of this function will be applied in the implementation of Deep Convolutional Generative Adversarial Networks. Some particularities of each type of these functions are the following :

- Sigmoid : takes any real value as input but always outputs a value between 0 and 1
- Hyperbolic Tangent : takes any real value as input and outputs values between -1 and 1
- Rectified Linear Unit (ReLU) function : takes any real value as input, outputs 0 if the input is negative and the value of the input if not

There exists also a variation of the ReLU function, called Leaky ReLU, which instead of computing 0 if the input is negative, it computes a value given by a linear function with negative values and negative slope. The slope coefficient is defined before training.

Each neuron also has a Bias component, which has a role in enriching the representation of the input through the weights of the model. The bias represents a constant, and its presence shifts the activation function towards the positive or the negative values. In the absence of a bias, a simple matrix multiplication between the inputs and weights is effectuated. This situation can result in the overfitting over the data set.

Below we can find an illustration of the architecture of a neural network :

![image](https://user-images.githubusercontent.com/114659655/209229886-8c6cc5c9-cf3b-4140-9d97-847437dddf90.png)

**source : Malik, F. (2019) : Neural Networks Bias And Weights (https://medium.com/fintechexplained/neural-networks-bias-and-weights-10b53e6285da#:~:text=The%20addition%20of%20bias%20reduces,to%20activate%20the%20activation%20function.)**


## Data 

The dataset used in our study is the Celeba Dataset. It represents a collection of more than 200K photos of celebrities, which come in .jpg format. We can plot some of the images in the dataset :
![image](https://user-images.githubusercontent.com/114659655/209142529-fabd6e5a-4f66-407f-b67c-fd4cffc5e7f3.png)



## Deep Convolutional Generative Adversarial networks 

From a general point of view, Convolutional Neural Networks represent a particular type of neural networks that posess an ability to detect patterns in the data (for example, shapes, corners or edges in the case of images). Their hidden layers, called "Convolutional Layers", receive input, transform it and then pass it to the following layers. The operation that corresponds to the transformation of an input is called a "Convolution".

### Convolution

The object at the basis of the convolution process is the kernel. It is represented by a 2-dimensional matrix of pixels that will be passed along the pixels of the input image. The values that constitute the kernel represent the weights of the network, they are randomly initialized and will be further optimized according to the accuracy and loss of the convolution process. The kernel we are using in our application is 4x4, chosen according to the paper Radford A., Metz L. Chintala S. (2016) : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, in order to match the dimensions desired for the feature maps passed through each layer. Its size is the same in the case of the Generator as in the case of the Discriminator.

The kernel is initialized and then passed along the image, row by row and column by column, until it has slid over all the pixels of the image. A dot product is applied between the kernel and the surface on the image on which it slides, resulting in a sum of element by element product between the kernel and the components of the image matrix. The resulting image is the output matrix. Each Kernel position corresponds to a single output pixel, for which the value is calculated by multiplying together the kernel value and the underlying image pixel value for each pixel in the kernel, and summing up the results. As it can be deduced, the size of the output image of the convolution process will be smaller, depending on the size of the initial image and the size of the kernel. In the case of a Convolutional Neural Network, the output obtained by the convolution inside a hidden layer is passed as input to the next hidden layer, after its normalization through the batch normalization layer and passage through the activation function. The process of convolution is illustrated below :

![1_VVvdh-BUKFh2pwDD0kPeRA@2x](https://user-images.githubusercontent.com/114659655/209142358-c8b6cd4e-294c-4125-a06d-8483b77f4744.gif)

**source : Dertat, A. (2017) : Applied Deep Learning - Part 4: Convolutional Neural Networks (https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)**

Stride and padding are the 2 other elements present in the process of convolution. When kernel is being passed along the image, it can be noticed that some pixels participate in more convolution operations than others. It is the case for the pixels in the center, compared to the ones on the corners. Hence, the corner pixels are less used in feature detection. Padding helps us solve this issue by adding rows and columns of empty pixels. Stride dictates how the kernel moves along the image. A stride of 1 means that the kernel is moving 1 by 1 pixel each time, while a stride of 2 means the kernel moves 2 by 2 pixels.

### Transposed Convolution

In our application, namely in the case of the Generator, we will come across Transposed Convolution. The purpose of such procedure is the opposite of the basic convolution, in the sense that it is looking to increase the size, height and width of the input image. Its functioning is also different.

The Transposed Convolution begins by padding the input image with zeros. This procedure happens without using the padding specified by us as argument, but using an implicit padding instead. Then, the kernel is convolved over the padded image with a stride of 1, also implicit and not representing a stride that we specify as argument. We can see an illustration of the process below :

![image](https://user-images.githubusercontent.com/114659655/209095000-6a92a857-d70e-43ce-aa52-bc5d8ade49cd.png)
**source : Godoy, D. (2022) : What are Transposed Convolutions ?**


Let us concentrate on the stride and padding that we introduce as arguments. Choosing a stride with a value superior to 1 will modify the process of transposed convolution. Zero-valued pixels will be introduced inbetween both the columns and the rows of the existing image. The stride introduced minus 1 will give the number of columns and rows to be inserted. For example, a stride of 2 will introduce 1 row and 1 column of zeros. Then, the padding is added as mentioned before and usual convolution with a fixed stride of 1 is applied.


### Convolutional Layers

The neural networks we are going to build are going to be formed of Convolutional Layers. Convolutional Layers are meant to process data that is correlated in space, meaning pixels in an image whose measurements are represented by their color value that goes from 0 to 255. The pixels are correlated as they are part of the same image, therefore the value of one pixel wil influence the value of any neighbouring pixel. This is what makes the data correlated in space, as opposed to data correlated in time, for example, such as time series. The output of each convolutional layer is a feature map, which corresponds to a spatial projection where certain features are exposed, which have previously been found by the convolutional layer.

The layers of the network are interconnected : the second layer takes as input the output from the first layer and so on. This can be seen in the values of the arguments that are passed onto each layer. For example, the number of output channels from the first layer will be the number of input channels of the second layer. We implement the layers using they torch.nn module.

### Batch Normalization Layers

Batch Normalization is applied right after the Convolutional Layer. It consists in the normalization of the activation vectors using the mean and standard deviation of the current bach. The term "normalization" refers to constraining the data to having values between 0 and 1. Given that the data points in an image are pixels, which have a value between 0 and 225, it is necessary to normalize them to a range that is manageable, otherwise the weights of the neural network will be affected during training by becoming too large and hence producing pixels on a very wide range of values for the output image. As mentioned in the paper by Radford A., Metz L. Chintala S. (2016) "[batch normalization] helps deal with training problems that arise due to poor initialization and helps gradient flow in deeper models".

### Activation functions

Activation functions take as input the output produced by Batch Normalization layers. The activation functions are used depending on the type of output we are looking for. We are defining the functions according to the paper by Radford A., Metz L. Chintala S. (2016) : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. For the generator, we are using ReLU activation functions, except for the output layer which uses Tanh. The choice is based on the fact that the authors observed that a bounded activation function such as Tanh allowed quicker learning during training for the network. For the discriminator, the authors used Leaky ReLU activations because of their good performance in contexts of images of high resolution. The chosen slope for the Leaky ReLU activation functions was 0.2. As outputs of the Discriminator should be probabilities and hence have values between 0 and 1, the activation function of the last layer of the discriminator wll be sigmoid.

## Generator and Discriminator

Throughout the implementation of our neural networks, we wil use the Pytorch framework. Both the Generator and the Discriminator are Neural Network Object Types. They are defined as classes who inherit from the nn.Module in Pytorch. The structure of both networks will be defined using the __init__() method and applied to computation using the forward() method. We are designing the networks by blocks of layers, each block containing a convolutional (or transposed convolutional for the generator) layer, a batch normalization layer and an activation function. The blocks will have positions after the first or before the last layer of each network.


### Generator

The Generator Neural Network's initial input is represented by a latent 1-dimensional noise vector. The length of the latent vector is fixed and can be chosen arbitrarily, but we will choose 100 for our example. We will draw the latent vector from the Gaussian distribution. 

![1_ULAGAYoGGr5eEB2377ArYA](https://user-images.githubusercontent.com/114659655/201658929-c53960f3-1d5d-4e33-b6c6-f6f88d484100.png)

**source : Inkawich, N - DCGAN Tutorial (https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)**


Transposed convolution is then applied by sliding the kernel along the noise vector. We need to pass from a 1x100 dimensional vector to the size of the images in the dataset (3x64x64, meaning 3 channels, 64 pixels height and 64 pixels width) for the final output, the convolution applied is transposed in order to upsample the data. Each transposed convolution will produce a feature map inside the generator.

Batch Normalization : After the Convolutional Layer, a batch normalization layer is implemented. It normalizes data at batch level so that it can be passed through the activation function afterwards. We are applying Batch Normalization to 2d images using BatchNorm2d().

The generator that we have implemented consists in 4 blocks of layers, followed by a Transposed Convolutional layer and a Hyperbolic Tangent activation function, following the architecture in the paper by Radford A., Metz L. Chintala S. (2016).


```
class Generator(nn.Module):

  """Generator class Network
  
  Class inherits from pytorch Neural Network Module
  We are taking the noise vector and then we are passing 2D Transposed Convolutional layers, that are paired with Batch Normalization Layers and a ReLU activation function.
  Class is composed by the __init__ function for initialization and a function __layers which corresponds to a block of a Convolutional 2D layer, a Batch Normalization Layer 
  and the ReLU activation function.

  Parameters

  ----------
  latent_dim : dimension of the latent noise vector
  channels : number of channels of the output image
  gen_features : size of feature maps (images obtained after applying the convolution) in the generator 
  ----------

  References 

  ----------

  [1^]  [Radford A., Metz L. Chintala S. (2016) : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks] (https://arxiv.org/abs/1511.06434)
  [2^]  [DCGAN Repository] (https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/2.%20DCGAN/model.py)

  """
  ### Pure python implementation can be found here
  ### https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/2.%20DCGAN/model.py


  def __init__(self, latent_dim, channels, gen_features): 
    super(Generator, self).__init__()
    self.gen = nn.Sequential(
       #Convolution of input Z
       self._layers(latent_dim, gen_features*16, 4, 1, 0),
      
       #Dimension gen_features*16 x 8 x 8
       self._layers(gen_features*16, gen_features*8, 4, 2, 1),

       #Dimension gen_features*8 x 16 x 16
       self._layers(gen_features*8, gen_features*4, 4, 2, 1),

       #Dimension gen_features*4 x 32 x 32
       self._layers(gen_features*4, gen_features*2, 4, 2, 1),


       #Dimension gen_features*2 x 64 x 64
       nn.ConvTranspose2d(gen_features*2, 
                         channels, 
                         kernel_size = 4, 
                         stride = 2, 
                         padding = 1
                          ),
      nn.Tanh()
      )
 
  #Creation of block of layers : Transposed Convolution, Batch Normalization, ReLU 
  def _layers(self, channels_input, channels_output, kernel_size, stride, padding): 
      return nn.Sequential(
          nn.ConvTranspose2d(channels_input, 
                             channels_output,
                             kernel_size,
                             stride, 
                             padding, 
                             bias = False),
          nn.BatchNorm2d(channels_output),
          nn.ReLU(),
      )


  def forward(self, x):
     return self.gen(x)
```

### Discriminator

The input of the discriminator is an image in its intial dimensions. The image is processed through Conv2d layers, as the data has to be downsampled. As mentioned before, the filter (Kernel) passes through the set of pixels of the image. Values of the corresponding pixels are multiplied together then summed up to result in the convolved feature. 


![image](https://user-images.githubusercontent.com/114659655/208300570-d18620da-22cf-4d00-a430-b6293ade3069.png)

**source : Tsang, S. (2022) : Review: DCGAN — Deep Convolutional Generative Adversarial Network (GAN) (https://sh-tsang.medium.com/review-dcgan-deep-convolutional-generative-adversarial-network-gan-ec390cded63c)**

The Discriminator is formed by a first Convolutional layer, followed by a Leaky ReLU activation function and 3 blocks of layers, each containing a Convolutinal Layer, a Batch Normalization layer and a Leaky ReLU activation function. The last convolution layer of the discriminator is flattened and passed through a sigmoid function. The Discriminator will therefore output the label of the image, 0 corresponding for 'fake' and 1 corresponding for 'real'. The following image illustrates the architecture of the Discriminator Network.


```
class Discriminator(nn.Module):

  """Discriminator Class Network

  Class inherits from pytorch Neural Network Module
  Takes an image as input and outputs the probability that the image is real by applying a series of Convolutional 2D, Batch Normalization and LeakyReLU layers using function _layers.
  Output is generated through a sigmoid function.

  Parameters

  ----------  
  channels : number of channels in the initial image
  disc_features : number of channels that are going to change as we are passing through the discriminator
  ----------

  References 

  ----------

  [1^]  [Radford A., Metz L. Chintala S. (2016) : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks] (https://arxiv.org/abs/1511.06434)
  [2^]  [DCGAN Repository] (https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/2.%20DCGAN/model.py)

  """
  ### Pure python implementation can be found here
  ### https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/2.%20DCGAN/model.py

  def __init__(self, channels, disc_features):
    super(Discriminator, self).__init__()
    
    #Input dimensions : channels x 64 x 64
    self.disc = nn.Sequential(
        #Dimension disc_features x 32 x 32
        nn.Conv2d(
            channels, 
            disc_features, 
            kernel_size = 4, 
            stride = 2, 
            padding = 1
        ),
        nn.LeakyReLU(0.2),
        
        #Dimension disc_features*2 x 16 x 16
        self._layers(disc_features, disc_features*2, 4, 2, 1),

        #Dimension disc_features*4 x 8 x 8
        self._layers(disc_features*2, disc_features*4, 4, 2, 1),

        #Dimension disc_features*8 x 4 x 4
        self._layers(disc_features*4, disc_features*8, 4, 2, 1),

        
        #Dimension of output : 1 x 1
        nn.Conv2d(disc_features*8, 1, 4, 2, 0),
        nn.Sigmoid()
    )
   
    #Creation of block of layers : convolution, normalization, LeakyReLU
  def _layers(self, channels_input, channels_output, kernel_size, stride, padding):
       return nn.Sequential(
         nn.Conv2d(
            channels_input,
            channels_output,
            kernel_size,
            stride,
            padding,
            bias = False
        ),
        nn.BatchNorm2d(channels_output),
        nn.LeakyReLU(0.2)
    )

  
  def forward(self, x):
      return self.disc(x)
```


## Weights initializiation

Model weights will be initialized according to the paper by Radford, Metz and Chintala (2015). They are normally distributed with mean 0 and standard deviation 0.02. The function weights_initialization takes as input one of the 2 models and transforms input data within the network's hidden layers, respectively the Convolutional, Transposed Convolutional and Batch Normalization Layer.

For a Convolutional Layer and Transposed Convolutional Layer, weights represent each matrix element in the Kernel, that will be trained in the training loop. The weights will be initialized after the the model intialization and before the training.


```
def weights_initialization(model):

  """Weights initialization function

  Takes model as input and initializes the weights in each of its layer
  Initialization of Normally distributed weights with mean 0 and standard deviation 0.2

  Parameters

  ----------  
  model : Generator or discriminator, taken as input
  ----------

  References 

  ----------

  [1^]  [Radford A., Metz L. Chintala S. (2016) : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks] (https://arxiv.org/abs/1511.06434)
  [2^]  [DCGAN Repository] (https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/2.%20DCGAN/model.py)


  """

  ### Pure python implementation can be found here
  ### https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/2.%20DCGAN/model.py
  for module in model.modules():
    if module == nn.Conv2d or module == nn.ConvTranspose2d or module == nn.BatchNorm2d :
      nn.init.normal_(model.weight.data, 0.0, 0.02)
```



# Training setup 

After the Discriminator and the Generator have been initialized given the hyperparameters defined at the beginning of the code (number of channels and number of features for the discriminator and size of the latent vector, the number of channels and the number of features for the generator), weights are initialized. We use the Binary Cross Entropy loss function and Adam Optimizer for both Networks. In order to understand the choice for these characteristics, we have to look at the principle of interaction between the networks :

![image](https://user-images.githubusercontent.com/114659655/208265282-205b6375-2370-408f-998e-27ed7c864ba5.png)

**source : Inkawich, N - DCGAN Tutorial (https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)**

Here D(G(z)) is the probability that the output of the Generator G is classified as a real image. Let us look now at the Binary Cross Entropy Loss : 

![image](https://user-images.githubusercontent.com/114659655/208265308-6345ba98-1718-48be-9d65-2294af4cc528.png)

**source : Inkawich, N - DCGAN Tutorial (https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)**

It can be noticed that the 2 functions look alike. Indeed, the value of y can be dictated so as to correspond to the objective function of each of the 2 networks. For the Generator, we are looking for min log(1 - D(G(z))) which is equivalent to max log(D(G(z)), whereas for the Discriminator we are looking for min log(D(x)) + log(1-D(G(z)). 

It is worth reminding that the function BCELoss measures the Binary Cross Entropy between an input tensor and a target tensor. In our model, labelling is done with the value 1 for images classified as real and 0 for images classified as fake by the discriminator, through the functions zeros_like and ones_like. The values are put in a tensor, which is compared to a tensor of the same size filled with 1's through the BCE Loss criterion. The difference between the 2 network consists in the tensor used as input for the BCELoss function.

### Optimizers and their intialization

The training for our Networks  on the CelebA dataset will use the Adam (Adaptative moment estimation) optimizer, an extension of the stochastic gradient descent, given its computation efficiency in large datasets. However, it is not the only choice and we will see later than in some cases other optimizers perform better.

Adam optimizer is an adaptative learning rate method, meaning that it computes individual learning rates for different parameters. It does so by using estimations of the first and second moments of the gradient (mean and uncentered variance) to adapt the learning rate for each weight in the neural network. The procces is done by the usage of exponentially moving averages computed on the current batch.

Hyperparameters for the Adam optimizer are the learning rate, beta1 and beta2. The values of the betas represent the coefficients of the exponentially moving averages. We will take their values suggested in Radford A., Metz L. Chintala S. (2016) : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. These values are a 0.0002 for the learning rate (found empirically by the authors, as the value 0.0001 was too high), 0.5 for beta1 and 0.999 for beta2. It should be noticed that conventional values for the betas are 0.9 and 0.999, but the authors found that a "value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped
stabilize training" (Radford A., Metz L. Chintala S. (2016) : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, p.4).


### Training loop

The Generator is trained to minimize the loss with respect to the fake images it generates. By looking at its loss function log(1 - D(G(z))), we can see that its minimization is equivalent to max(D(G(z))), which will be the optimization problem in our training loop. The Discriminator's loss has 2 components as we have seen in its formula. We are maximizing the probability that it labels real images as real and fake images as fake. Through each batch the following operations are applied :

- A number of real images equal to the batch size are taken from the dataset and and sent to CUDA
- A number of random noise vectors equal to the batch size is created and inputted to the Generator for the creation of fake images
- Training of the Discriminator : On the real images, Discriminator classifies the image as "real" or "fake", according it the value 0 for "fake" and "1" for real. The obtained result is flattened into a 1 dimensional vector with values 0 or 1, which is compared to a vector composed only of value 1's the same size as the one obtained after the flattening. Criterion of loss BCE is applied in order to measure this comparison. The same procedure is applied on the fake images, but this time the obtained tensor for comparison is composed only of value 0's. The obtained BCE loss function, which will be the sum of the 2 losses, is optimized through the Adam optimizer
- Training of the Generator : Discriminator network is applied to the generated fake images. The result of this operation is flattened on a 1-dimensional vector composed of values 0 or 1 depending on the classification by the discriminator. This obtained vector is compared through the BCE to a vector of the same size full of 1's. The obtained loss function will be minimzed through the Adam Optimizer, as we want fake image to be classified as real as often as possible.

```
"""Training Loop with alternative learning rates

Initialization of Generator and Discriminator, initialization of their weights and optimizers
Initialization of Binary Cross-Entropy as loss function
Initialization of empty lists of losses
Usage of lr/4 for the generator and 0.00000138 for the discriminator as learning rates

Passage through each batch and each epoch and optimization of objective functions for the discriminator and the generator at each iteration. 
Printing of the average accuracy of the discriminator
Once every 100 iterations, images are added to the image list.

Objects

----------
disc : discriminator
gen : generator
gen_optimizer : Adam optimizer taking generator's parameters as input, as well as its hyperparameters
disc_optimizer : Adam optimizer taking discriminator's parameters as input, as well as its hyperparameters
criterion : Binary Cross Entropy Loss Function
step : counter for the number of iterations
D_losses : list of calculated losses at each iterations for the discriminator
G_obj : list of calculated values for the objective function at each iteraton for the generator
img_list : list of generated images

References 

----------

[1^]  [Radford A., Metz L. Chintala S. (2016) : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks] (https://arxiv.org/abs/1511.06434)
[2^]  [Inkawich, N - DCGAN Tutorial] (https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
[3^]  [DCGAN Repository] (https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/2.%20DCGAN)


"""
### Pure python implementation can be found here
### https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/2.%20DCGAN/train.py


#Networks initialization with given hyperparameters
disc = Discriminator(n_channels, disc_features).to(device)
gen = Generator(size_latent, n_channels, gen_features).to(device)

#Weights initialization with mean 0 and sd 0.2
weights_initialization(disc)
weights_initialization(gen)

#Adam optimizers initialization for Discriminator and Generator with beta velus corresponding to the paper
gen_optimizer = optim.Adam(gen.parameters(), lr = lr/4, betas = (beta1, beta2))
disc_optimizer = optim.Adam(disc.parameters(), lr = 0.00000138, betas = (beta1, beta2))

#Binary Cross-Entropy loss criterion
criterion = nn.BCELoss()

#Training step
step = 0

#Initial noise vector initialization for further comparison
init_noise = torch.randn(32, size_latent, 1, 1).to(device)


#Putting generator and discriminator in training mode
gen.train()
disc.train()

#List of losses
D_losses = []
G_losses = []
img_list = []

#Training Loop

#For each epoch 
for epoch in range(epochs):
  #For each batch in the dataLoader
  for batch, (real_image, target) in enumerate(dataloader):
    print("Batch number " + str(batch))
    
    #Uploading real image to gpu
    real_image = real_image.to(device)

    #Generating noise vectors
    noise = torch.randn((batch_size, size_latent, 1, 1)).to(device)
    
    #Generating fake image from noise
    fake = gen(noise)

    ###Train discriminator###

    #Discriminator loss on real image : reshaping to have a single value for each image
    disc_real = disc(real_image).reshape(-1)

    #Loss function computation
    loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))

    #Discriminator accuracy on real images : proportion of correctly predicted real images
    pred_reals = torch.sum(disc_real)
    length_real = disc_real.size(dim=0)
    acc_real = pred_reals/length_real


    #Discriminator loss on fake image : reshaping to have a single value for each image
    disc_fake = disc(fake).reshape(-1)
    loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))

    #Discriminator accuracy on fake images : proportion of correctly predicted fake images
    pred_fakes = torch.sum(disc_fake)
    length_fake = disc_fake.size(dim=0)
    acc_fake = (length_fake - pred_fakes)/length_fake

    #Average discriminator accuracy
    disc_accuracy = (acc_real + acc_fake)/2
    print("Discriminator Average Accuracy :" + str(disc_accuracy))


    #Total discriminator loss
    disc_loss = (loss_disc_real + loss_disc_fake)/2

    D_losses.append(disc_loss.item())
    print("Discriminator Loss function:" + str(disc_loss.item()))

    #Update discriminator
    disc.zero_grad()
    
    #Calculate gradient for discriminator in backward pass
    #Setting retain graph = True in order to allow re-utilisation of fake in the Generator optimization part
    disc_loss.backward(retain_graph = True)

    #Update discriminator
    disc_optimizer.step()

    ###Train generator###


    #Vector of discriminator predictions on fake images : reshaping to have a single value for each image
    output = disc(fake).reshape(-1)

    #Computing generator loss function
    generator_loss = criterion(output, torch.ones_like(output))
    gen.zero_grad()

    #Calcuate gradients for generator
    generator_loss.backward()

    #Update generator
    gen_optimizer.step()

    #Adding loss value for generator in loss list and printing it
    G_losses.append(generator_loss.item())
    print("Generator objective function :" + str(generator_loss))


    #Appending images in the image list every 100th step :
    if step % 100 == 0:
      
      #Generating fake images from initial noise with optimized generator
      with torch.no_grad():
        fake = gen(init_noise).detach().cpu()
        #Appending list 
        img_list.append(utils.make_grid(fake, normalize = True))
    #Incrementing number of steps
    step = step + 1

```

### Results

Let us look at some of the obtained images after 3 epochs :

![image](https://user-images.githubusercontent.com/114659655/209209427-4f39756c-a68d-4a63-a3b6-1b3382c72af3.png)

The images look very noisy. We cannot conclude on the performance of our networks based on this result. Let us take a look at the loss functions for the generator and the discriminator. As the authors mention in the paper by Radford A., Metz L. Chintala S. (2016) : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, instability often is present in the training of GANs.

```
"""Plot of obtained images throught imshow


References 

----------

[1^]  [Inkawich, N - DCGAN Tutorial] (https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)

"""
### Pure python code can be found here
### https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html

fig = plt.figure(figsize=(8,8))
plt.axis("off")
ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]
```



Let us now look at the graph of the evolution of the objective functions for both the Generator and the Discriminator :

![image](https://user-images.githubusercontent.com/114659655/209132815-a01c3cce-ecef-4d8b-a450-3ad308c854b7.png)


We can see that the loss function for the discriminator is going down very quickly as the number of iterations increases, while the objective function for the Generator is actually going up. This explains the noisy result that we have obtained. Indeed, the Discriminator is much more performant than the Generator. The code for the graph plot is found below :

```
"""Plot of objective functions

Loss function should be decreasing for the Discriminator, while the objective function should be increasing

References 

----------

[1^]  [Inkawich, N - DCGAN Tutorial] (https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)

"""

#Graph of objective functions
plt.figure(figsize = (6,6))
plt.title("Objective functions evolution")
plt.plot(G_losses, label = "Generator objective function")
plt.plot(D_losses, label = "Discriminator objective function")
plt.xlabel("Iterations")
plt.legend()
plt.show()
```

We have to balance the performance of the Generator and the Discriminator. One of the solutions for this issue is to change the learning rate so that the discriminator learns more slowly. Empirically, we have found that a learning rate of 0.00000138 for the discriminator and a division of the initial learning rate by 4 for the generator induces more stability in the training. Let us look at the obtained loss functions with this learning rate after 1 epoch:

![image](https://user-images.githubusercontent.com/114659655/209312508-1b832161-ee66-4604-b816-f99f4e12d4a3.png)



The results look more stable, the Generator is learning better and is able to beat the discriminator to a certain point. We can plot some of the images obtained from this training loop :

![image](https://user-images.githubusercontent.com/114659655/209312618-159b5be5-1aba-4673-94c4-e8f3a76ade21.png)


We can notice that the images are a bit more suggestive this time, even from the first epoch.



## Extension : application on the MNIST Dataset

In order to see the behavior of our Discriminator and Generator, we can apply them to the MNIST dataset. A few differences are present at the level of hyperparameters. MNIST images are only black and white, meaning they have only 1 channel instead of 3. The image size is 32 instead of 64 here and we are using a batch size of 32 as the dataset is smaller. The networks are trained in the same manner as before. We can take a look at the images obtained after 3 epochs :

![image](https://user-images.githubusercontent.com/114659655/209212572-214cbe4d-0f23-436c-ac9e-4318d98d01bb.png)


The learning rates used here were the initial learning rates of 0.0002 given by the authors. As the networks have not been trained enough, images produced are very noisy. We can now take a look at the objective functions of the 2 networks :

![image](https://user-images.githubusercontent.com/114659655/209212613-bfcac060-5852-4f1c-bb6e-ef8f31716c7d.png)

We can see this time that the 2 functions are very close together. The Generator's objective function is increasing whereas the Discriminator's loss function is decreasing, but not at the same pace as before. A solution to this issue can be the change in learning rates as we did before. Afterwards, more training will lead to better results.

The implementation of DCGANs on the MNIST dataset can be found in the file DCGAN_MNIST.ipynb file in the src/ folder.



# Extension : WGAN- Wasserstein Generative Adversarial Networks

We have seen the principles of GANs and the functioning of DCGANS. We are now focusing on Wasserstein Generative Adversarial Networks


## Introduction and concept

Firstly let us understand why such variations of the traditional GANs have been created. While a very performant tool, GANs can be subject to convergence failure (failure to produce optimal results) and mode collapse (model failing to produce unique results and repeating a similar pattern, quality or classes). The advantage of WGANs is that they solve this issue and they offer higher stability for the training in comparison to traditional GANs. Another advantage is that the value of the global loss is meaningful, in the sense that it gives us a termination criterion, contrary to the loss of classic GANs.

We have seen that the main idea in GAN implementation is that we have 2 probability distributions, one for the Generator Pg and one for the Discriminator Pd. The goal is to have similar probability distributions so that generated images look realistic. The issue here is therefore how we define this similarity, or the "distance" between the 2 probability distributions. In the case of WGAN's, we are focusing on Wasserstein distance measure. 

![image](https://user-images.githubusercontent.com/114659655/209140474-d8b16a4c-9380-4347-ae12-712015f2c301.png)

**source : [Arjovsky M., Chintala S., Bottou L., (2017): Wasserstein GAN] (https://arxiv.org/abs/1701.07875)**

Here the capital PI represents the set of all joint distributions whose marginal distributions are Pr respectively Pg. The other name of this distance measure is "Earth Mover distance", which comes from the fact that it measures the "energy" needed to shift one probability distribution towards another one. We are using the term "energy" as the Wasserstein distance measure gives the minimum product between the mass of the distribution to be shifted and the distance along which it has to be moved. This measure of divergence is different from the one present in classical Generative Adversarial Networks, which is called "Jensen Shannon Divergence" (JS) and takes the form that we have seen in the previous section. One main difference between the Wasserstein and the Jensen Shannon Divergence measure is that the Wasserstein divergence measure is a horizontal measure, contrary to JS. To be more precise, it corresponds to the distance between densities on the X-axis, while the JS divergence is measured on the Y-axis. One main advantage of the Wasserstein distance is that it is differentiable almost everywhere, which will allow us to reach optimality while training. In addition to this, the value of the Wasserstein distance is more meaningful, in the sense that the closer the distributions get, it converges to 0.

## The Critic

The Discriminator in the case of WGANS takes the name of "critic". Its functioning is similar to the Discriminator we have seen in the case of GANs, but its objective function is different. While the discriminator is trained to correctly identify samples from the 2 distributions Pg and Pd, the critic is used to predict the distance between the 2 distributions.

In order to implement the critic, we are taking the code of the Discriminator we have defined before and remove the Sigmoid function. We are doing so as the role of the critic is no longer to give a binary classification.


```
class Critic(nn.Module):

 """Critic implementation

  Critic neural network implementation from the Discriminator, without Sigmoid function


  Parameters

  ----------  
  channels : number of channels in the initial image
  disc_features : number of channels that are going to change as we are passing through the discriminator
  ----------

  References 

  ----------


  [1^] [Arjovsky M., Chintala S., Bottou L., (2017): Wasserstein GAN] (https://arxiv.org/abs/1701.07875)
  [2^] [WGAN Repository] (https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/3.%20WGAN/model.py)

  """
  ### Pure python implementation can be found here
  ### https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/3.%20WGAN/model.py


 def __init__(self, channels, disc_features):
    super(Critic, self).__init__()
    
    #Input dimensions : channels x 64 x 64
    self.critic = nn.Sequential(
        #Dimension disc_features x 32 x 32
        nn.Conv2d(
            channels, 
            disc_features, 
            kernel_size = 4, 
            stride = 2, 
            padding = 1
        ),
        nn.LeakyReLU(0.2),
        
        #Dimension disc_features*2 x 16 x 16
        self._layers(disc_features, disc_features*2, 4, 2, 1),

        #Dimension disc_features*4 x 8 x 8
        self._layers(disc_features*2, disc_features*4, 4, 2, 1),

        #Dimension disc_features*8 x 4 x 4
        self._layers(disc_features*4, disc_features*8, 4, 2, 1),

        
        #Dimension of output : 1 x 1
        nn.Conv2d(disc_features*8, 1, 4, 2, 0),
    )
   
    #Creation of block of layers : convolution, normalization, LeakyReLU
 def _layers(self, channels_input, channels_output, kernel_size, stride, padding):
       return nn.Sequential(
         nn.Conv2d(
            channels_input,
            channels_output,
            kernel_size,
            stride,
            padding,
            bias = False
        ),
        nn.BatchNorm2d(channels_output),
        nn.LeakyReLU(0.2)
    )

  
 def forward(self, x):
      return self.critic(x)
```


## The Generator

We have seen that the role of the generator in classical GANs (including DCGANs) is to produce fake data as optimally as possible by having its objective loss function minimized. In the case of WGANs, the generator seeks to minimize the distance between the 2 distributions Pg and Pd.

## Training WGANs

In the training cell, we are intializing the parameters the way we did in the GAN section. This time however we are taking the parameters metioned in the paper by Arjovsky M., Chintala S., Bottou L., (2017): Wasserstein GAN. We are taking a learning rate of 0.00005 and batch size is 64. 2 additional parameters are added, the discriminator iterations and the weight clip. Weight clipping prevents the gradient from getting too large in training, making the model unstable. The idea of weight clipping is that if the gradient gets too large, we rescale the parameters by the value of the weight clip.



Below we find the training loop of a WGAN :

```
"""Wasserstein GAN

Usage of a different optimization method than the first DCGANs implemented

Objects

----------
disc : discriminator
gen : generator
gen_optimizer : Adam optimizer taking generator's parameters as input, as well as its hyperparameters
disc_optimizer : Adam optimizer taking discriminator's parameters as input, as well as its hyperparameters
criterion : Binary Cross Entropy Loss Function
step : counter for the number of iterations
D_losses : list of calculated losses at each iterations for the discriminator
G_obj : list of calculated values for the objective function at each iteraton for the generator
img_list : list of generated images


References 

----------

[1^] [Arjovsky M., Chintala S., Bottou L., (2017): Wasserstein GAN] (https://arxiv.org/abs/1701.07875)
[2^] [WGAN Repository] (https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/3.%20WGAN/train.py)

"""

### Pure python code can be found here
### https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/3.%20WGAN/train.py


disc2 = Critic(n_channels, disc_features).to(device)
gen2 = Generator(size_latent, n_channels, gen_features).to(device)

#Weights initialization with mean 0 and sd 0.2
weights_initialization(disc2)
weights_initialization(gen2)

#Enabling training mode for critic and generator
disc2.train()
gen2.train()

#Hyperparameters for training setup
lr = 0.00005
batch_size = 64
latent_dim = 100
epochs = 3

#Additional hyperparameters for WGAN : critic iterations and weight clip
disc_iterations = 5
weight_clip = 0.01

#Optimizers for WGAN
gen_optimizer = optim.RMSprop(gen2.parameters(), lr = lr)
disc_optimizer = optim.RMSprop(disc2.parameters(), lr = lr)

G_losses = []
D_losses = []
img_list = []
 

#Training Loop
for epoch in range(epochs):
  for batch, (real_image, _) in enumerate(dataloader):
    print("Batch number " + str(batch))
    
    #Uploading real image to gpu
    real_image = real_image.to(device)

    #additional loop for additional critic training
    for _ in range(disc_iterations):

      #Generating noise vectors
      noise = torch.randn((batch_size, size_latent, 1, 1)).to(device)
    
      #Generating fake image from noise
      fake = gen2(noise)

      ###Train discriminator###


      #Discriminator on real image
      disc_real = disc2(real_image).reshape(-1)
      loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))

      #Discriminator on fake image
      disc_fake = disc2(fake).reshape(-1)

      #Loss of discriminator maximizing distance but we are through a minimization algorithm
      #Therefore we are minimizing the opposite of the value
      loss_disc = -(torch.mean(loss_disc_real) - torch.mean(loss_disc_fake))

      #adding critic loss to list and printing it
      D_losses.append(loss_disc.item())
      print("Discriminator Loss :" + str(loss_disc.item()))


      #Discriminator Adam optimization
      disc.zero_grad()

      #Setting retain_graph = True as we will re-utilise the fake data generated
      disc_loss.backward(retain_graph = True)
      disc_optimizer.step()

      #Going through critic parameters clamping
      for parameter in disc.parameters():
        parameter.data.clamp_(-weight_clip, weight_clip)

      ###Train generator###

      #Discriminate fake and get loss
      output = disc2(fake).reshape(-1)

      #Computing generator loss
      generator_loss = criterion(output, torch.ones_like(output))
      
      #Printing and adding to list of generator loss
      print("Generator Loss :" + str(generator_loss.item()))
      G_losses.append(generator_loss.item())

      #Generator Adam optimization
      gen.zero_grad()
      generator_loss.backward()
      gen_optimizer.step()

      #Adding images to list
      if batch % 100 == 0:

        with torch.no_grad():
          fake = gen2(init_noise).detach().cpu()
          img_list.append(utils.make_grid(fake, padding = 2, normalize = True))
      step = step + 1


```


# References

[1^]  [Inkawich, N - DCGAN Tutorial] (https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)

[2^]  [Radford A., Metz L. Chintala S. (2016) : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks] (https://arxiv.org/abs/1511.06434)

[3^] [Arjovsky M., Chintala S., Bottou L., (2017): Wasserstein GAN] (https://arxiv.org/abs/1701.07875)

[4^] [WGAN Repository] (https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/3.%20WGAN)

[5^] [Godoy, D. (2022) : What are Transposed Convolutions ?] (https://towardsdatascience.com/what-are-transposed-convolutions-2d43ac1a0771#:~:text=Transposed%20convolutions%20are%20like%20the,the%20generator%20part%20of%20GANs.)

[6^] [Tsang, S. (2022) : Review: DCGAN — Deep Convolutional Generative Adversarial Network (GAN)] (https://sh-tsang.medium.com/review-dcgan-deep-convolutional-generative-adversarial-network-gan-ec390cded63c)

[7^] [Dertat, A. (2017) : Applied Deep Learning - Part 4: Convolutional Neural Networks] (https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)

[8^]  [DCGAN Repository] (https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/2.%20DCGAN)

[9^]  [Malik, F. (2019) : Neural Networks Bias And Weights] (https://medium.com/fintechexplained/neural-networks-bias-and-weights-10b53e6285da#:~:text=The%20addition%20of%20bias%20reduces,to%20activate%20the%20activation%20function.)

[10^] [Goodfellow I., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y. (2014) : Generative Adversarial Networks] (https://arxiv.org/abs/1406.2661)





